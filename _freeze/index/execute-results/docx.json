{
  "hash": "98cf895212c82ac99e4cec4e077ddcb3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Unravelling mental representations in aphantasia through unsupervised alignment\nsubtitle: Project design and data analysis simulation\nauthor: Maël Delem\nabstract-title: Summary\nabstract: |\n  Research on aphantasia is confronted with a long-standing conundrum of all research on consciousness and representations, namely the theoretical inaccessibility of subjective representations. Drawing on concepts from similarity and representation research, I endorse the view that the study of an individual’s mental representations is made possible by exploiting second-order isomorphism. The concept of second-order isomorphism means that correspondence should not be sought in the first-order relation between (a) an external object and (b) the corresponding internal representation, but in the second-order relation between (a) the perceived similarities between various external objects and (b) the similarities between their corresponding internal representations. Building on this idea, this study project report is divided into five parts. **First**, I outline the central ideas underlying similarity research and its applicability to aphantasia research. **Second**, I present a methodological rationale and protocol based on inverse multidimensional scaling that can be implemented online to conduct such large-scale research with high efficiency. **Third**, I present a data analysis plan using a state-of-the-art method for similarity analysis, unsupervised alignment with Gromov-Wasserstein optimal transport (GWOT). **Fourth**, I report a data simulation of a potential outcome of this project and the successful analysis of this synthetic data using GWOT alignment. **Fifth**, I analyse the feasability of such a project given the material constraints of my thesis. I conclude with the expected utility and benefits of this project.\nbibliography: references.bib\n---\n\n\n\n::: {.callout-tip collapse=\"true\"}\n# Project inception\n\nThis project stems from several elements:\n\n1.  The long standing knowledge of the fact that internal representations seem impossible to reach due to their subjective nature.\n\n2.  The discovery of the article of @shepardSecondorderIsomorphismInternal1970 that expose the idea of \"second-order isomorphism\".\n\n3.  The discovery of state-of-the-art and accessible unsupervised analytic methods to study this principle in an astonishing way. The last two discoveries (and many more) are the fruit of amazing discussions and recommendations from Ladislas when he came here. These motivated me to try to implement GWOT in R on data that I wanted to create myself to emulate a study we could do.\n\n*I promise that I did this mostly on my spare time, we have too many other things to do elsewhere.*\n:::\n\n# Theoretical context\n\n## Work in progress\n\n## Psychological spaces and aphantasia\n\nWhile attempting to demonstrate the uselessness of the concept of similarity as a philosophical and scientific notion[^1], @goodmanSevenStricturesSimilarity1972 has inadvertently expressed an aspect of similarity judgements of primary importance to us aphantasia researchers:\n\n[^1]: A claim dismissed since then by propositions of robust mathematical models of similarity, e.g. @gardenforsConceptualSpacesFramework2004, @decockSimilarityGoodman2011.\n\n> Comparative judgments of similarity often require not merely selection of relevant properties but a weighting of their relative importance, and variation in both relevance and importance can be rapid and enormous. Consider baggage at an airport checking station. The spectator may notice shape, size, color, material, and even make of luggage; the pilot is more concerned with weight, and the passenger with destination and ownership. Which pieces are more alike than others depends not only upon what properties they share, but upon who makes the comparison, and when. . . . Circumstances alter similarities.\n\nThis can be easily reversed as an argument in favor of the **potential of similarity analyses to highlight the inter-individual differences in sensory mental representations**. For example, should we ask individuals to judge the similarities in shape or color between various objects, the *differences between the similarity structures* of individuals will be precisely the most important phenomenon for us, far less than the constancy between these structures. If we can account for the context dependence, as we will propose here with explicit instructions, clever task design, and hypothesis-neutral analysis, we could overcome the limitations of the inherently subjective nature of similarity judgements.\n\nThis idea of a difference in similarity judgements in aphantasia seems to transpire in the results of @bainbridgeQuantifyingAphantasiaDrawing2021 on their drawing study. They have shown that aphantasics had more schematic representations during recall, accurate in their spatial positioning, but with less sensory details. This difference can be seen from two perspectives: (1) a memory deficit for sensory properties; (2) a different representational structure of the items in their psychological spaces. In the latter case, aphantasics would have greater/faster abstraction of their representation of a perceived scene, reducing the amount of encoded sensory details unconsciously considered to be relevant. Both (1) and (2) can theoretically explain the same behavioural response, i.e. less sensory elements and correct spatial recall accuracy in aphantasic drawings, but **the two have drastically different consequences on how we define, characterize, and judge aphantasia.**\n\nThe dominant hypothesis seems to be that aphantasics simply have an episodic or general memory deficit. Conversely, I hypothesize that aphantasics have different representational structures than phantasics in certain dimensions of their psychological spaces (notably sensory, but potentially abstract too). More generally, I hypothesize that the concept of visual imagery evaluates in reality the continuous spectrum of representational structures in *sensory* dimensions of psychological spaces. Mirroring visual imagery, spatial imagery could also be a rough psychometric evaluation of the continuous spectrum of structural differences in *conceptual/abstract* dimensions of psychological spaces. In this view, the psychological space of aphantasics would constrain internal representations to particularly abstract forms from a very early stage, thus selectively limiting the item properties thereafter encoded in long-term memory. In other terms, **I hypothesize that aphantasia would not be characterized by an episodic memory deficit, but by an episodic memory *selectivity* caused by the specific characteristics of their representational structures and psychological spaces.** This selectivity would have, as we already hypothesized several times, benefits and drawbacks.\n\n@gardenforsConceptualSpacesFramework2004 proposed that differences in psychological (in his terms, conceptual) spaces could arise from various sources, whether innate, due to learning, or broader cultural or social differences. All these hypotheses could be coherent to explain the sources of aphantasia. Nevertheless, the study of these sources should be the subject of very large-scale or longitudinal studies, which are out of the scope of this project.\n\nHere, we shall rather attempt to **develop a method to characterize the differences in aphantasics' representational structures and psychological spaces.**\n\n# Methodology\n\n@roads2024, in a recent review on the state and perspectives of similarity research, highlighted two challenges that studies in this field had to face: (1) The high cost of collecting behavioral data on a large number of stimuli; (2) The lack of software packages being a high barrier to entry, making the task of coding models difficult for the uninitiated.\n\nTo solve these problems, we present here two solutions, respectively for (1) experimental design and (2) data analysis:\n\n1.  A recent method to efficiently acquire similarity judgements, the \"multiple arrangement of items\" and \"inverse multidimensional scaling\" developed by @kriegeskorteInverseMDSInferring2012.\n2.  An accessible and robust Python toolbox provided by @sasakiToolboxGromovWassersteinOptimal2023 to conduct unsupervised alignment analysis using Gromov-Wasserstein optimal transport.\n\n## Experimental design\n\n### Multi-arrangement and inverse multidimensional scaling\n\nAssuming a geometric model of representational similarities, @kriegeskorteInverseMDSInferring2012 developed a multi-arrangement (MA) method to efficiently acquire (dis)similarity judgments for large sets of objects. The subject has to perform multiple arrangements of item subsets adaptively designed for optimal measurement efficiency and for estimating the representational dissimilarity matrix (RDM) by combining the evidence from the subset arrangements.\n\nThe procedure is illustrated in @fig-multi-arrangement.\n\n![**Acquiring similarity judgements with the multi-arrangement method. (A)** Subjects are asked to arrange items according to their similarity, using mouse drag-and-drop on a computer. The similarity measure is taken as the distances between the items: similar items are closer, while dissimilar items are further apart. The upper part of the figure shows screenshots at different moments of the acquisition for one subject. Columns are trials and rows show the object arrangements over time, running from the start (top row) to the end (last row). The first trial contains all items; subsequent trials contain subsets of items that are adaptively selected to optimally estimate judged similarity for each subject. **(B)** Once acquisition of the final judgements is completed, inter-item distances in the final trial arrangements are combined over trials by rescaling and averaging to yield a single dissimilarity estimate for each object pair. The process is illustrated in this figure for two example item pairs: a boy's face and a hand (red), and carrots and a stop sign (blue). Their single-trial dissimilarity estimates (arrows) are combined into a single dissimilarity estimate, which is placed at the corresponding entry of the RDM (lower panel). Mirror-symmetric entries are indicated by lighter colors [figure from @murHumanObjectSimilarityJudgments2013].](images/multi-arrangement-method-mur-2013.png){#fig-multi-arrangement .column-body}\n\nA key strength of this method that sets it as particularly effective is the \"adaptive\" part. The goal of the process is to acquire similarity judgements as precisely as possible while minimizing the total amount of trials. To do so, starting from the second trial, selected subsets of the items to be compared are presented to the subject: these items are the ones that were very close on-screen in previous trials and thus had their distance evaluated with lower accuracy by the subject. As the subject has to fill the entire \"arena\" with the items, these subsequent trials will necessarily increase the level of precision in the similarity judgement between pairs of items. The second key benefit of this method is the time and effort gain compared to others. For example, to compare every pair of items among 64 different items would require $\\frac{64 \\times (64-1)}{2} = 2016$ comparisons (i.e. trials). This would be extremely time-consuming, while also losing the *context-independence* afforded by the MA method due to the presence of other items around every time the subject mentally performs a pairwise comparison.\n\nHistorically, when referring to the projection of the representations of stimuli (e.g., coordinates in geometric space) from a high-dimensional space into a lower-dimensional space, inference algorithms were commonly called multidimensional scaling [@roads2024]. By analogy, the process of combining several lower-dimensional (2D) similarity judgements on-screen to form one higher dimensional similarity representation (in the RDM) can be conceptually seen as \"inverse\" multidimensional scaling, hence the name given to the method by @kriegeskorteInverseMDSInferring2012.\n\n### Principle\n\nThe idea is simple: for a given set of items that have distinct and very pictorial visual properties, we would ask a wide range of aphantasics, phantasics or hyperphantasics to imagine, mentally compare and make similarity judgements between the items. To compare these representations with actual perceptual representations, the subjects would also perform the same task afterwards, this time with actual pictures to compare. Subjects would also fill our usual psychometric imagery questionnaires.\n\nTo \"compare imagined items\", we could use a \"word\" version of the MA paradigm. An example from @majewskaSpatialMultiarrangementClustering2020 - *who used the method to build large-scale semantic similarity resources for Natural Language Processing systems* - is represented in @fig-majewska.\n\n![Arena layout of the MA protocol used by to acquire similarity judgements on word pairs [figure from @majewskaSpatialMultiarrangementClustering2020].](images/majewska-spam.png){#fig-majewska width=\"50%\"}\n\nWe could have the stimuli rated by another set of participants on several features.\n\n> « *We deliberately did not specify which object properties to focus on, to avoid biasing participants' spontaneous mental representation of the similarities between objects. Our aim was to obtain similarity judgments that reflect the natural representation of objects without forcing participants to rely on one given dimension. However, participants were asked after having performed the task, what dimension(s) they used in judging object similarity.* » [@jozwik2016]\n\n> « ***All but one of the 16 participants reported arranging the images according to a categorical structure.*** » [@jozwik2017]\n\nThis result of @jozwik2017 suggests that we should give an explicit instruction about the features to focus on, otherwise everyone might bypass visual features and mental images in favour of concepts and categories, regardless of their mental imagery profile.\n\nIn contrast, if we ask to focus specifically on the visual features, then ask subjects about the strategy they used to evaluate the similarities, then on the subjectively felt mental format of these strategies, we might grasp better insight on the sensory representations of subjects.\n\nWe could even go for several comparisons - even though this would increase quadratically the number of trials - e.g. :\n\n-   Evaluate to what extent the **shape** *of these animals are* ***similar*** **at rest, ignoring size differences.**\n\n-   Evaluate to what extent these animals **sound like each other.**\n\n-   Etc.\n\n> *Note to be added: if you do not know the animal, just guess its placement, as this situation is quite unlikely to happen (animals chosen are fairly common knowledge).*\n\n@kawakita2023: To assess whether the color dissimilarity structures from different participants can be aligned in an unsupervised manner, we divided color pair similarity data from a large pool of 426 participants into five participant groups (85 or 86 participants per group) to obtain five independent and complete sets of pairwise dissimilarity ratings for 93 color stimuli (Fig. 3a). Each participant provided a pairwise dissimilarity judgment for a randomly allocated subset of the 4371 possible color pairs. We computed the mean of all judgments for each color pair in each group, generating five full dissimilarity matrices referred to as Group 1 to Group 5.\n\n### Stimuli\n\nWe would have a list of animal items, that would have several characteristics:\n\n-   A name\n\n-   A category\n\n-   A shape\n\nWe need orthogonal data:\n\n-   Each class of animal should include each shape (roughly)\n\n-   Each shape should have an animal\n\nThis would imply that category cannot be derived from shape, and vice-versa. Thus, a **sorting by shape would reveal to be innately visual** (or maybe spatial, if shape concerns this type of imagery), and a **sorting by category would reveal an abstraction** from these shapes. We expect that the two will be mixed to some degree in every subject, but that low-imagery would rather tend towards category sorting, while high-imagery would tend towards shape sorting.\n\nShapes could be very tricky stimuli to discuss. @gardenfors2004 noted that we only have a very sketchy understanding of how we perceive and conceptualize things according to their shapes. The works of @marr1997 highlight this difficulty when analysing the complexity of the hierarchical judgements of shapes and volumes, as shown in @fig-marr.\n\n![Representing the characteristics of shapes with cylinders [figure from @marr1997].](images/shapes-marr.png){#fig-marr}\n\n# Data analysis plan\n\n## Unsupervised alignment rationale\n\nVisual images can be represented as points in a multidimensional psychological space. Embedding algorithms can be used to infer latent representations from human similarity judgments. While there are an infinite number of potential visual features, an embedding algorithm can be used to identify the subset of salient features that accurately model human-perceived similarity. (*From Roads' CV*)\n\nUsing an optimization algorithm, the free parameters of a psychological space are found by maximizing goodness of fit (i.e., the loss function) to the observed data. Historically, when referring specifically to the free parameters that correspond to the representation of stimuli (e.g., coordinates in geometric space), inference algorithms were commonly called multidimensional scaling (MDS), or simply scaling, algorithms.\n\nIn the machine learning literature, analogous inference algorithms are often called embedding algorithms. The term \"embedding\" denotes a higher-dimensional representation that is embedded in a lower-dimensional space. For that reason, the inferred mental representations of a psychological space could also be called a psychological embedding.\n\nNumerous techniques exist, and each has limitations. Popular techniques for comparing representations include RSA @kriegeskorte2008 and canonical correlation analysis (CCA) (Hotelling 1936). Briefly, RSA is a method for comparing two representations that assesses the correlation between the implied pairwise similarity matrices. CCA is a method that compares two representations by finding a pair of latent variables (one for each domain) that are maximally correlated.\n\nOne might be tempted to compare two dissimilarity matrices assuming stimulus-level \"external\" correspondence: my \"red\" corresponds to your \"red\"(Fig. 1d). This type of supervised comparison between dissimilarity matrices, known as Representational Similarity Analysis (RSA), has been widely used in neuroscience to compare various similarity matrices obtained from behavioural and neural data. However, there is no guarantee that the same stimulus will necessarily evoke the same subjective experience across different participants. Accordingly, when considering which stimuli evoke which qualia for different individuals, we need to consider all possibilities of correspondence: my \"red\" might correspond to your \"red\", \"green\", \"purple\", or might lie somewhere between your \"orange\" and \"pink\"(Fig. 1e). Thus, we compare qualia structures in a purely unsupervised manner, without assuming any correspondence between individual qualia across participants.\n\n## Gromov-Wasserstein optimal transport\n\nTo account for all possible correspondences, we use an unsupervised alignment method for quantifying the degree of similarity between qualia structures. As shown in Fig. 2a, in unsupervised alignment, we do not attach any external (stimuli) labels to the qualia embeddings. Instead, we try to find the best matching between qualia structures based only on their internal relationships (see Methods). After finding the optimal alignment, we can use external labels, such as the identity of a color stimulus (Fig. 2b), to evaluate how the embeddings of different individuals relate to each other. This allows us to determine which color embeddings correspond to the same color embeddings across individuals or which do not. Checking the assumption that these external labels are consistent across individuals allows us to assess the plausibility of determining accurate inter-individual correspondences between qualia structures of different participants.\n\nTo this end, we used the Gromov-Wasserstein optimal transport (GWOT) method, which has been applied with great success in various fields. GWOT aims to find the optimal mapping between two point clouds in different domains based on the distance between points within each domain. Importantly, the distances (or correspondences) between points \"across\" different domains are not given while those \"within\" the same domain are given. GWOT aligns the point clouds according to the principle that a point in one domain should correspond to another point in the other domain that has a similar relationship to other points. The principle of the method is illustrated in @fig-gwot-kawa\n\n![Gromov-Wassertein optimal transport principle [figure from @kawakita2023].](images/kawa-gwot-2.PNG){#fig-gwot-kawa}\n\nWe first computed the GWD for all pairs of the dissimilarity matrices of the 5 groups (Group 1-5) using the optimized $\\epsilon$. In Fig. 3b, we show the optimized mapping $\\Gamma*$ between Group 1 and Groups 2-5 (see Supplementary Figure S1 for the other pairs). As shown in Fig. 3b, most of the diagonal elements in $\\Gamma*$ show high values, indicating that most colors in one group correspond to the same colors in the other groups with high probability. We next performed unsupervised alignment of the vector embeddings of qualia structures. Although $\\Gamma*$ provides the rough correspondence between the embeddings of qualia structures, we should find a more precise mathematical mapping between qualia structures in terms of their vector embeddings to more accurately assess the similarity between the qualia structures. Here, we consider aligning the embeddings of all the groups in a common space.\n\nBy applying MDS, we obtained the 3-dimensional embeddings of Group 1 and Groups 2-5, referred to as X and Yi, where i = 2, ..., 5 (Fig. 3c). We then aligned Yi to X with the orthogonal rotation matrix Qi, which was obtained by solving a Procrustes-type problem using the optimized transportation plan $\\Gamma*$ obtained through GWOT (see Methods). Fig. 3d shows the aligned embed- dings of Group 2-5 (QiYi) and the embedding of Group 1 (X) plotted in the embedded space of X. Each color represents the label of a corresponding external color stimulus. Note that even though the color labels are shown in Fig. 3d, this is only for the visualization purpose and the whole alignment procedure is performed in a purely unsupervised manner without relying on the color labels. As can be seen in Fig. 3d, the embeddings of similar colors from the five groups are located close to each other, indicating that similar colors are 'correctly' aligned by the unsupervised alignment method.\n\nTo evaluate the performance of the unsupervised alignment, we computed the k-nearest color matching rate in the aligned space. If the same colors from two groups are within the k-nearest colors in the aligned space, we consider that the colors are correctly matched. We evaluated the matching rates between all the pairs of Groups 1-5. The averaged matching rates are 51% when k = 1, 83% when k = 3, and 92% when k = 5, respectively. This demonstrates the effectiveness of the GW alignment for correctly aligning the qualia structures of different participants in an unsupervised manner.\n\nHowever, as can be seen in Fig. 4b, the optimized mapping $\\Gamma*$ is not lined up diagonally unlike the optimized map- pings between color-neurotypical participants groups shown in Fig. 3b (see Supplementary Figure S1 for the other pairs). Accordingly, top k matching rate between Group 1-5 and Group 6 is 3.0% when k = 1 (Fig. 4c), which is only slightly above chance ($\\approx$ 1%). The matching rate did not improve even when we relaxed the criterion (6.9% and 11% for k = 3 and k = 5, respectively). Moreover, all of the GWD values between Group 1-5 and Group 6 are larger than any of the GWD values between color-neurotypical participant groups (Fig. 4d).\n\nThese results indicate that the difference between the qualia structures of neuro-typical and atypical participants is significantly larger than the difference between the qualia structures of neuro-typical participants.\n\n![The two conditions for one subject.](images/my-protocol-1.png)\n\n![The comparison between the representational structure of aphantasics and phantasics. This figure illustrates the principle, but in reality all pairs of subjects will be compared to assess their representational structure alignment. This is computationnally heavy, but analytically very powerful.](images/my-protocol-2.png)\n\n<!-- Summary schematics of the proposed experimental protocol and data analysis plan. -->\n\n## Hypotheses\n\n### Aphantasic and phantasic psychological spaces\n\nThe most representative members of a category are called prototypical members.\n\nPrototype theory builds on the observation that among the instances of a property, some are more representative than others. The most representative one is the prototype of the property.\n\nThus, following the concepts illustrated by @gardenfors2004, we would expect that aphantasics, when doing shape similarity judgements, would be more inclined to group items close to the prototypical items due to a lower definition of the mental image. In comparison, phantasics would have a much more distributed conceptual space of item shapes due to their higher-resolution mental images of said items.\n\n### Subjective imagery and psychological spaces\n\nIn the proposed view of visual imagery as the subjective expression of a given type of psychological space, we mentioned earlier that *spatial* imagery could also constitute a subjective expression of other dimensions of psychological spaces. Hence, the *verbal* dimension of the simplified model of imagery we outlined in my thesis project could also represent different dimensions.\n\nThis conception leads to the following theoretical hypothesis: provided that our visual-spatial-verbal model correctly fits subjective imagery, the imagery profile of individuals should map on their psychological spaces.\n\nOperationally, this would be evaluated by the fact that **individuals with similar imagery profiles** (visual, spatial, verbal, or any combination of the three) **should have similar representations** in their given psychological space, **quantifiable by the degree of alignment between their similarity structures.**\n\n# Study simulation and analysis\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ═══ Packages ═════════════════════════════════════════════════════════════════\n\nif (!require(librarian)) install.packages(librarian) \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLe chargement a nécessité le package : librarian\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(librarian)                                     \n\n# now putting packages on our library's shelves:\nshelf(\n  # ─── data management ─────────────────\n  holodeck,       # simulating multivariate data\n  cluster,        # dissimilarity matrices\n  \n  # ─── modelling ───────────────────────\n  mclust,         # mixture clustering\n  \n  #  data visualization ──────────────\n  # palettes\n  viridis,        # colour-blind friendly palettes\n  # interactive\n  plotly,         # interactive plots\n  ggdendro,       # dendrograms\n  seriation,      # dissimilarity plots\n  webshot2,       # HTML screenshots for Word render\n  webshot,\n  \n  # ─── essential package collections ───\n  doParallel,     # parallel execution\n  easystats,      # data analysis ecosystem\n  reticulate,     # R to Python                    \n  tidyverse,      # modern R ecosystem\n)\n\n# ─── Global cosmetic theme ───\ntheme_set(theme_modern(base_size = 14))\n\npal_okabe_ito <- c(     # <3>                                                \n  \"#E69F00\", \"#56B4E9\", \"#009E73\",                            \n  \"#F5C710\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")      \n\n# We'll need 9 colors at some point\npal_okabe_ito_extended <- c(                                 \n  \"#E69F00\", \"#56B4E9\", \"#009E73\",                           \n  \"#F5C710\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\", \"#414487FF\")\n\n# We'll need 30 colors at another moment\ncool_30_colors <- c(                                                   \n  \"#3d51b4\", \"#414487FF\", \"#003d73\", \"#440154FF\", \"#6c0009\", \"#b64e4e\",\n  \"#D55E00\", \"#E69F00\", \"#F5C710\", \"#FDE725FF\", \"#f2bb7b\", \"#f1afad\", \"#CC79A7\", \n  \"#e57774\", \"#7AD151FF\", \"#57b571\", \"#318a4a\", \"#009E73\", \"#22A884FF\", \n  \"#2A788EFF\", \"#0072B2\", \"#2da6b5\", \"#56B4E9\", \"#889be0\", \"#6677e0\",   \n  \"#3d51b4\", \"#414487FF\", \"#003d73\", \"#440154FF\", \"#6c0009\", \"#b64e4e\"  \n  ) # <3>                                                                      \n\npath = \"notebooks/data/\"\n\ndf <- read_rds(paste0(path, \"df.RDS\"))\ndf_embeds <- read_rds(paste0(path, \"df_embeds.RDS\"))\n# Categorical and visual embeddings\ndf_embeds_categ  <- read_rds(paste0(path, \"df_embeds_categ.RDS\"))\ndf_embeds_visual <- read_rds(paste0(path, \"df_embeds_visual.RDS\"))\n# Subject embeddings per sub-group\ndf_embed_c_sub  <- read_rds(paste0(path, \"df_embed_c_sub.RDS\"))\ndf_embed_cs_sub <- read_rds(paste0(path, \"df_embed_cs_sub.RDS\"))\ndf_embed_v_sub  <- read_rds(paste0(path, \"df_embed_v_sub.RDS\"))\ndf_embed_vs_sub <- read_rds(paste0(path, \"df_embed_vs_sub.RDS\"))\n# Accuracy of the unsupervised alignment (bad = not tidy data)\ndf_accuracy_all_bad <- read_rds(paste0(path, \"df_accuracy_all_bad.RDS\"))\ndf_accuracy_cat_bad <- read_rds(paste0(path, \"df_accuracy_cat_bad.RDS\"))\n# Coordinates of the aligned embeddings from the Python output\ncoordinates_aligned_embeddings <- read_rds(paste0(path, \"coordinates_aligned_embeddings.RDS\"))\n```\n:::\n\n\n\n## Visual-spatial-verbal model of cognitive profiles\n\nOne of the objectives of the study would be to link the subjective cognitive profiles of individuals with their representational structures. To evaluate these profiles, we are going to use psychometric questionnaires evaluating the visual-object, spatial, and verbal dimensions of imagery which will yield three scores, one for each dimension.\n\nWe are going to simulate 30 participants presenting four different cognitive profiles, that I defined as, respectively, *verbal* aphantasics, *spatial* aphantasics, *spatial* phantasics, and *visual* phantasics. Their imagery abilities are summarised in @tbl-imageries.\n\nTo simulate these four sub-groups, we use the `holodeck` R package to generate multivariate normal distributions of scores on these three dimensions for each sub-group. For instance, verbal aphantasics have normally distributed visual imagery scores centered around a mean of 0 (normalized, so negative scores are possible), 0.4 for spatial imagery, and 0.7 for verbal style; Spatial aphantasics have means of 0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are arbitrary, but have been chosen by trial-and-error to obtain a model that is both well-defined and not exaggerated. The 30 subjects' imagery profiles are represented in the three dimensional space of the visual-spatial-verbal dimensions in @fig-plot-osv-model.\n\n<!-- The code used to generate these scores can be found in the Study simulation notebook. -->\n\n| Cognitive profile  | Visual imagery | Spatial imagery | Verbal style |\n|--------------------|:--------------:|:---------------:|:------------:|\n| Verbal aphantasic  |       --       |       \\-        |      ++      |\n| Spatial aphantasic |       --       |       ++        |      \\-      |\n| Spatial phantasic  |       \\+       |       ++        |      \\-      |\n| Visual phantasic   |       ++       |       \\-        |      \\+      |\n\n: Imagery abilities of the four hypothesized cognitive profiles. {#tbl-imageries}\n\n\n\n::: {#cell-fig-plot-osv-model .cell}\n\n```{.r .cell-code .hidden}\nplotting_osv_model <- function(df, grouping_variable, size){\n  df |> \n    plot_ly(\n      x = ~visual_imagery,\n      y = ~spatial_imagery,\n      z = ~verbal_profile,\n      color = ~df[[grouping_variable]],\n      text  = ~df[[grouping_variable]],\n      colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\"),\n      type = \"scatter3d\",\n      mode = \"markers+text\",\n      marker = list(size = size),\n      textfont = list(size = size + 4)\n    ) |> \n    layout(\n      scene = list(\n        xaxis = list(\n          title = list(text = \"Visual imagery\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          ),\n        yaxis = list(\n          title = list(text = \"Spatial imagery\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          ),\n        zaxis = list(\n          title = list(text = \"Verbal profile\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          )\n      ),\n      legend = list(title = list(text = \"Group\")),\n      paper_bgcolor = \"transparent\"\n      )\n}\n\ndf |> \n  mutate(vis_spa_group = case_when(\n    vis_spa_group == \"aph_spa_high\" ~ \"Aph. spatial\",\n    vis_spa_group == \"aph_spa_low\"  ~ \"Aph. verbal\",\n    vis_spa_group == \"phant_spa_high\" ~ \"Phant. spatial\",\n    vis_spa_group == \"phant_spa_low\"  ~ \"Phant. visual\"\n  )) |> \n  plotting_osv_model(grouping_variable = \"vis_spa_group\", size = 4)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nPhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Imagery profiles generated for 30 subjects on the three object, spatial, and verbal dimensions.](index_files/figure-docx/fig-plot-osv-model-1.png){#fig-plot-osv-model}\n:::\n:::\n\n\n\n## Data simulation: Creating representational structures\n\n@gardenfors2004 invokes two scientific concepts, to wit, prototypes and Voronoi tessellations. Prototype theory builds on the observation that among the instances of a property, some are more representative than others. The most representative one is the prototype of the property. *We hypothesize that aphantasics will be more inclined to categorize items according to prototypes than phantasics.*\n\nA Voronoi tesselation of a given space divides that space into a number of cells such that each cell has a center and consists of all and only those points that lie no closer to the center of any other cell than to its own center; the centers of the various cells are called the generator points of the tesselation. This principle will underlie our data simulation, as we will build representations in a 3D space based on distances to \"centroids\", namely, prototypes. These representations will thus be located inside of the tessellations around these prototypes, more or less close to the centroid depending on the subject's representational structures.\n\n### Generating \"prototype\" embeddings from a sphere\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# getting the centroids of each cluster\ndf_centroids <- \n  df_embeds |> \n  group_by(group) |> \n  summarise(\n    x_centroid = mean(x),\n    y_centroid = mean(y),\n    z_centroid = mean(z)\n  )\n\n# adding them to the data\ndf_embeds_2 <- left_join(df_embeds, df_centroids, by = \"group\")\n```\n:::\n\n\n\nA function will be used to generate embeddings. These spherical embeddings are displayed in @fig-perfect-embeddings We get 8 nicely distributed clusters. We'll retrieve the centroids of each cluster, which would be the \"perfect\" categories of each species group (say, generated by a computational model on categorical criteria).\n\n\n\n::: {#cell-fig-perfect-embeddings .cell}\n\n```{.r .cell-code .hidden}\n# function for 3D plotting up to 8 groups (due to the palette)\nplotting_3d <- function(df, size, opacity){\n  df |> \n    plot_ly(\n      type = \"scatter3d\",\n      mode = \"markers\",\n      x = ~x,\n      y = ~y,\n      z = ~z,\n      color = ~paste0(\"Species group \", group),\n      colors = pal_okabe_ito,\n      marker = list(size = size, opacity = opacity)\n    ) |> \n    layout(paper_bgcolor = \"transparent\")\n}\n\nplotting_3d(df_embeds, 3, 1) |> \n  layout(legend = list(\n    yanchor = \"top\",\n    y = 1,\n    xanchor = \"right\",\n    x = 0\n    ))\n```\n\n::: {.cell-output-display}\n![Generated spherical distribution of 1000 observations grouped in 8 equal clusers with Gaussian Mixture Clustering to represent the theoretical embeddings of 8 groups (i.e. groups of species here). ***Interact with the figures to see the details.***](index_files/figure-docx/fig-perfect-embeddings-1.png){#fig-perfect-embeddings}\n:::\n:::\n\n::: {#cell-fig-centroids .cell}\n\n```{.r .cell-code .hidden}\ndf_centroids |> \n  plot_ly(\n    type = \"scatter3d\",\n    mode = \"markers+text\",\n    x = ~x_centroid,\n    y = ~y_centroid,\n    z = ~z_centroid,\n    text = ~paste0(\"Species group \", group),\n    color = ~paste0(\"Species group \", group),\n    colors = pal_okabe_ito,\n    marker = list(size = 12, opacity = 1)\n    ) |> \n  layout(\n    scene = list(\n      xaxis = list(title = \"x\"),\n      yaxis = list(title = \"y\"),\n      zaxis = list(title = \"z\")\n    ),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Centroids of the 8 clusters created on the sphere, thus representing the prototypical embeddings of 8 groups (i.e. groups of species here). ***Interact with the figures to see the details.***](index_files/figure-docx/fig-centroids-1.png){#fig-centroids}\n:::\n:::\n\n\n\nNow we want two sets of embeddings: one where the observations are very concentrated around the centroids, which would be the **categorical model**, and one where the observations are more spread out, which would be the **visual model**.\n\nWe need to select 8 observations per cluster, which would be our animals per group. These observations will be subsets of the 1000 observations we generated.\n\n### Categorical model embeddings\n\nThe selection procedure for the **categorical model** will consist of selecting points that are rather *close to the centroids*. Thus, we will filter the observations of the large sets to keep only points for which the distance to the centroid is inferior to a given value. That is, points for which the Euclidean norm of the vector from the observation to the centroid:\n\n$$d(centroid, observation) = \\sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}$$\n\nThis can be done using the function `norm(coordinates, type = \"2\")` in R.\n\n\n\n::: {#cell-fig-categorical-embeddings .cell}\n\n```{.r .cell-code .hidden}\n# Plotting these observations\nplotting_3d(df_embeds_categ, 6, 1)\n```\n\n::: {.cell-output-display}\n![Selection of 64 points to represent prototypical categorical embeddings, based on the distances to each groups' centroid. These will be the bases of the verbal aphantasics' embeddings. ***Interact with the figure to see the details.***](index_files/figure-docx/fig-categorical-embeddings-1.png){#fig-categorical-embeddings}\n:::\n:::\n\n\n\n### Visual model embeddings\n\nIn the case of the **visual model**, we would like approximately evenly distributed embeddings, that could also dive *inside* the sphere, i.e. representing species that are visually close although diametrically opposed when it comes to taxonomy. To do this we could try to simulate multivariate normal distributions around the centroids[^2]. This can be done with the `holodeck` package.\n\n[^2]: A simpler alternative would be generating the visual embeddings with the same code as the categorical ones, selecting 8 points per cluster but much more spread out (e.g. selecting 8 among the 90% closest to the centroids, which would create more variability than the categorical one set to 60%). I chose otherwise because this wouldn't have had points reaching *inside* the sphere.\n\n\n\n::: {#cell-fig-visual-embeddings .cell}\n\n```{.r .cell-code .hidden}\nplotting_3d(df_embeds_categ, 4, 1) |> \n  add_trace(\n      data = df_embeds_visual,\n      type = \"scatter3d\",\n      mode = \"markers\",\n      x = ~x,\n      y = ~y,\n      z = ~z,\n      color = ~paste0(\"Species group \", group),\n      colors = pal_okabe_ito,\n      marker = list(size = 4, opacity = 1, symbol = \"diamond\")\n  )\n```\n\n::: {.cell-output-display}\n![Selection of 64 points to represent prototypical visual embeddings, chosen randomly in multivariate distributions centered around each categorical embedding. The visual embeddings are overlaid as diamonds along with categorical ones as dots. The two distributions keep the group structure, but are pretty far apart at times. ***Interact with the figure to see the details.***](index_files/figure-docx/fig-visual-embeddings-1.png){#fig-visual-embeddings}\n:::\n:::\n\n\n\n### Intermediate embeddings\n\n\n\n```{mermaid}\n%%| label: fig-distances-graph\n%%| fig-cap: \"Model of the distances between participants' representations. Note that here d is a one-dimensional distance between the representations, but it will be computed as a three-dimensional distance in our toy-model. The verbal aphantasic profile is hypothesized to be very categorical, thus diametrically opposed to the visual phantasic profile, by a given distance d. Spatial profiles are in-between: they are close to each other (10% x d), but the spatial aphantasic profile is a bit closer to the verbal aphantasic one (45% x d), and the spatial phantasic is a bit closer to the visual phantasic one (45% x d).\"\n%%| fig-width: 7\n\nflowchart LR\n    A(Aph. verbal) -----|0.45 x d| B(Aph. spatial) ---|0.10 x d| C(Phant. spatial) ------|0.45 x d| D(Phant. visual)\n    A ---|d| D\n```\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ─── Generating visual embeddings based on multivariate distributions around \n# the categorical embeddings ───\n\ndist_c = 0.45\ndist_v = 0.55\n\ndf_embeddings <-\n  df_embeds_categ |>\n  rename(\n    group_c = group,\n    x_c = x,\n    y_c = y,\n    z_c = z\n  ) |>\n  bind_cols(df_embeds_visual) |>\n  rename(\n    group_v = group,\n    x_v = x,\n    y_v = y,\n    z_v = z\n  ) |> \n  select(!group_v) |> \n  rename(group = group_c) |> \n  mutate(\n    x_cs = x_c + dist_c*(x_v - x_c),\n    y_cs = y_c + dist_c*(y_v - y_c),\n    z_cs = z_c + dist_c*(z_v - z_c),\n    x_vs = x_c + dist_v*(x_v - x_c),\n    y_vs = y_c + dist_v*(y_v - y_c),\n    z_vs = z_c + dist_v*(z_v - z_c)\n  )\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nNew names:\n• `species` -> `species...2`\n• `species` -> `species...7`\n```\n\n\n:::\n:::\n\n::: {#cell-fig-intermediate-embeddings .cell}\n\n```{.r .cell-code .hidden}\nsize = 3\n\ndf_embeddings |> \n  plot_ly(\n    type = \"scatter3d\", \n    mode = \"marker\",\n    color  = ~paste0(\"Species group \", group),\n    colors = ~pal_okabe_ito\n    ) |> \n  add_markers(\n    x = ~x_c, y = ~y_c, z = ~z_c, \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  add_markers(\n    x = ~x_v, y = ~y_v, z = ~z_v, \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond\")\n  ) |> \n  add_markers(\n    x = ~x_cs, y = ~y_cs, z = ~z_cs, \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle-open\")\n  ) |> \n  add_markers(\n    x = ~x_vs, y = ~y_vs, z = ~z_vs, \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond-open\")\n  ) |> \n  layout(\n    scene = list(\n      xaxis = list(title = \"x\"),\n      yaxis = list(title = \"y\"),\n      zaxis = list(title = \"z\")\n    ),\n    paper_bgcolor = \"transparent\")\n```\n\n::: {.cell-output-display}\n![Space of embeddings with 128 additional points based on the euclidean distances between the visual and categorical embeddings. The empty dots are the *aphantasics-spatial* ones, and the empty diamonds are the *phantasic-spatial* ones. Some can be very close together, and sometimes further apart due to the various pairs of visual and categorical points used to create them. A network-like structure seems to appear, with empty points seemingly 'connecting' the dots and diamonds. ***Interact with the figure to see the details.***](index_files/figure-docx/fig-intermediate-embeddings-1.png){#fig-intermediate-embeddings}\n:::\n:::\n\n\n\n### Labelling the species\n\nThe distributions created are still gathered around the centroids of each group, but are much more widespread, each group getting close to each other and even reaching inside the sphere.\n\nPerfect! Now we have two 3D embeddings per animal, in a categorical or a visual description of their features. Let's add labels for each species in a group:\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ─── Labeling each species in each group ───\n\ndf_embeddings <- \n  df_embeddings |> \n  mutate(\n    group = case_when(\n    group == 1 ~ \"a\", \n    group == 2 ~ \"b\",\n    group == 3 ~ \"c\",\n    group == 4 ~ \"d\",\n    group == 5 ~ \"e\",\n    group == 6 ~ \"f\",\n    group == 7 ~ \"g\",\n    group == 8 ~ \"h\",\n    TRUE ~ group\n    )\n  ) |> \n  group_by(group) |> \n  mutate(\n    species = paste0(\"species_\", group, 1:8),\n    species = as.factor(species),\n    group   = as.factor(group)\n    ) |> \n  select(group, species, everything())\n```\n:::\n\n\n\nNow we have four sets of coherent coordinates, that we need to assign to the 30 participants: i.e. generating 8 points for C (aph_spa_low), 7 points for CS (aph_spa_high), 7 points for VS (phant_spa_high), and 8 points for V (phant_spa_low).\n\n### Generating the subject embeddings\n\nWe have four \"reference\" sets of embeddings which represent animals either judged according to their similarity in categorical terms (namely, species), or in visual terms (namely shape or color similarities, assuming that these similarities are more evenly distributed, e.g. the crab looks like a spider, but is also pretty close to a scorpion, etc.).\n\nTo generate the embeddings of each subject in each condition, we will start from these reference embeddings and generate random noise around *each item*, i.e. for all 64 animals. For 100 subjects, we would thus generate 100 noisy points around each animal, each point corresponding to a given subject.\n\nThe visual and verbal groups will be generated with slightly more intra-group variance, so as to try to make the spatial groups as coherent as possible (and avoid blurring everything and making the groups disappear in noise).\n\nAlthough the groupings in this distribution sound simple when we color it using the knowledge about how we built it, the algorithm will only be fed with the data for each subject, without any labeling or additional information. Thus, @fig-subject-embeddings-b here is what the algorithm will \"see\" (and what it will try to decrypt). Admittedly, that looks a lot more complicated.\n\n\n\n::: {#cell-fig-subject-embeddings-a .cell}\n\n```{.r .cell-code .hidden}\nsize = 2\n\nplot_ly(\n    type = \"scatter3d\", \n    mode = \"marker\",\n    colors = ~pal_okabe_ito\n    ) |> \n  add_markers(\n    data = df_embed_c_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group),\n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  add_markers(\n    data = df_embed_v_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group),\n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond\")\n  ) |> \n  add_markers(\n    data = df_embed_cs_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group), \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle-open\")\n  ) |> \n  add_markers(\n    data = df_embed_vs_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group), \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond-open\")\n  ) |> \n  layout(legend = list(\n    yanchor = \"top\",\n    y = 1,\n    xanchor = \"right\",\n    x = 0\n    ),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Final distribution of the 64 embeddings of all the 30 subjects, amounting to 1920 points total. Embeddings are ***colored by the species groups*** they represent. The symbols represent the four imagery groups (Aph. verbal, spatial, etc.). ***Interact with the figures to see the details.***](index_files/figure-docx/fig-subject-embeddings-a-1.png){#fig-subject-embeddings-a}\n:::\n:::\n\n::: {#cell-fig-subject-embeddings-b .cell}\n\n```{.r .cell-code .hidden}\nbind_rows(\n  # aph_spatial\n  df_embed_cs_sub |> \n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_a_aph_s\", number), .keep = \"unused\"),\n  \n  # aph_verbal\n  df_embed_c_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_b_aph_v\", number), .keep = \"unused\"),\n  \n  # phant spatial\n  df_embed_vs_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_c_phant_s\", number), .keep = \"unused\"),\n  \n  # phant visual\n  df_embed_v_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_d_phant_v\", number), .keep = \"unused\")\n  ) |>\n  plot_ly() |> \n  add_markers(\n      x = ~x, y = ~y, z = ~z,\n      color = ~subject,\n      colors = cool_30_colors,\n      marker = list(size = 3)\n    ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Final distribution of the 64 embeddings of all the 30 subjects, amounting to 1920 points total. Embeddings are ***lored by subject***. The symbols represent the four imagery groups (Aph. verbal, spatial, etc.). ***Interact with the figures to see the details.***](index_files/figure-docx/fig-subject-embeddings-b-1.png){#fig-subject-embeddings-b}\n:::\n:::\n\n\n\nTo feed this data to the algorithm, we'll group the 64 embeddings per subject in matrices tied to each of them.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ─── Grouping the 64 embeddings of each subject in their own dataframe ───\n\ndf_embeddings_sub <-\n  bind_rows(\n    # aphantasic spatial\n    df_embed_cs_sub |> \n      separate_wider_delim(\n        subject,\n        delim = \"_\",\n        names = c(\"subject\", \"number\")\n      ) |> \n      mutate(subject = paste0(subject,\"_a_aph_s\", number), .keep = \"unused\"),\n    \n    # aphantasic verbal\n    df_embed_c_sub  |>\n      separate_wider_delim(\n        subject,\n        delim = \"_\",\n        names = c(\"subject\", \"number\")\n      ) |> \n      mutate(subject = paste0(subject,\"_b_aph_v\", number), .keep = \"unused\"),\n    \n    # phantasic spatial\n    df_embed_vs_sub  |>\n      separate_wider_delim(\n        subject,\n        delim = \"_\",\n        names = c(\"subject\", \"number\")\n      ) |> \n      mutate(subject = paste0(subject,\"_c_phant_s\", number), .keep = \"unused\"),\n    \n    # phantasic visual\n    df_embed_v_sub  |>\n      separate_wider_delim(\n        subject,\n        delim = \"_\",\n        names = c(\"subject\", \"number\")\n      ) |> \n      mutate(subject = paste0(subject,\"_d_phant_v\", number), .keep = \"unused\")\n  ) |> \n  mutate(subject = as.factor(subject)) |> \n  select(!c(group, species)) |> \n  group_by(subject) |> \n  nest() |> \n  rename(embedding = data) |> \n  rowwise() |> \n  mutate(embedding = list(as.matrix(embedding)))\n```\n:::\n\n\n\n## Data analysis: Aligning representational structures\n\n------------------------------------------------------------------------\n\n## Simulation summary\n\n@kawakita2023: These results indicate that the difference between the qualia structures of neuro-typical and atypical participants is significantly larger than the difference between the qualia structures of neuro-typical participants.\n\nA notable difference is that greenish colors and reddish colors are close in the embedding space of color atypical participants while they are distant in the embedding space of color neurotypical participants. This structural difference is likely to prevent the unsupervised alignment between the embeddings of color-neurotypical and atypical participants even though the correlation coefficient between the dissimilarity matrices of color neuro-typical and atypical participants is reasonably high.\n\nFor a long time, assessing the similarity of subjective experiences across participants has been challenging. To address this problem, we proposed the \"qualia structure\" paradigm, which focuses on quantitative structural comparisons of subjective experiences. Using an unsupervised alignment method, we were able to match the qualia structures of colors and natural objects of different groups of participants based only on the way the qualia relate to each other, without using any external labels.\n\nOur results on color qualia structures are consistent with an idea that the relational properties of color qualia are universally shared by color-neurotypical individuals. Intriguingly, our results also suggest that individuals with color-atypical vision may have a different structure of their color experiences, rather than just failing to experience a certain subset of colors. Longstanding thought experiments that challenge the feasibility of inter-subjective color comparisons, such as individuals with color qualia inversion, should be resolvable with our relational unsupervised approach. Beyond traditional measures such as Pearson's correlation coefficient, our method provides a more fundamental structural characterization of how two structures are similar or different, which will be crucial for future investigations of qualia structures across psychological, neuroscientific, and computational fields.\n\n# Feasibility\n\n# Conclusion\n\nModern psychology builds on the relativistic framework of philosophy, accepting that humans cannot know reality in an absolute sense. Focusing on relative comparisons, or similarity, is more than a clever philosophical work-around. similarity is a common currency of perception and cognition. In addition to operating at all levels of cognition, similarity---or, more accurately, the second-order isomorphism defined by a set of similarity relations---has been a powerful tool for analyzing and comparing psychological spaces.\n",
    "supporting": [
      "index_files\\figure-docx"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}