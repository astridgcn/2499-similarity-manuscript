{
  "hash": "be43a434c5cf11283595b74b62a15ea9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Unravelling mental representations in aphantasia through unsupervised alignment\nsubtitle: Project design and data analysis simulation\nauthor: Maël Delem\nabstract-title: Summary\nabstract: |\n  Research on aphantasia is confronted with a long-standing conundrum of all research on consciousness and representations, namely the theoretical inaccessibility of subjective representations. Drawing on concepts from similarity and representation research, I endorse the view that the study of an individual’s mental representations is made possible by exploiting second-order isomorphism. The concept of second-order isomorphism means that correspondence should not be sought in the first-order relation between (a) an external object and (b) the corresponding internal representation, but in the second-order relation between (a) the perceived similarities between various external objects and (b) the similarities between their corresponding internal representations. Building on this idea, this study project report is divided into five parts. **First**, I outline the central ideas underlying similarity research and its applicability to aphantasia research. **Second**, I present a methodological rationale and protocol based on inverse multidimensional scaling that can be implemented online to conduct such large-scale research with high efficiency. **Third**, I present a data analysis plan using a state-of-the-art method for similarity analysis, unsupervised alignment with Gromov-Wasserstein optimal transport (GWOT). **Fourth**, I report a data simulation of a potential outcome of this project and the successful analysis of this synthetic data using GWOT alignment. **Fifth**, I analyse the feasability of such a project given the material constraints of my thesis. I conclude with the expected utility and benefits of this project.\n# code reading options\ncode-fold: true\nbibliography: references.bib\n---\n\n\n::: {.callout-tip collapse=\"true\"}\n# Project inception\n\nThis project stems from several elements:\n\n1.  The long standing knowledge of the fact that internal representations seem impossible to reach due to their subjective nature.\n\n2.  The discovery of the article of @shepardSecondorderIsomorphismInternal1970 that expose the idea of \"second-order isomorphism\".\n\n3.  The discovery of state-of-the-art and accessible unsupervised analytic methods to study this principle in an astonishing way. The last two discoveries (and many more) are the fruit of amazing discussions and recommendations from Ladislas when he came here. These motivated me to try to implement GWOT in R on data that I wanted to create myself to emulate a study we could do.\n\n*I promise that I did this mostly on my spare time, we have too many other things to do elsewhere.*\n:::\n\n# Theoretical context\n\n## Psychological spaces and aphantasia\n\nWhile attempting to demonstrate the uselessness of the concept of similarity as a philosophical and scientific notion[^1], @goodmanSevenStricturesSimilarity1972 has inadvertently expressed an aspect of similarity judgements of primary importance to us aphantasia researchers:\n\n[^1]: A claim dismissed since then by propositions of robust mathematical models of similarity, e.g. @gardenforsConceptualSpacesFramework2004, @decockSimilarityGoodman2011.\n\n> Comparative judgments of similarity often require not merely selection of relevant properties but a weighting of their relative importance, and variation in both relevance and importance can be rapid and enormous. Consider baggage at an airport checking station. The spectator may notice shape, size, color, material, and even make of luggage; the pilot is more concerned with weight, and the passenger with destination and ownership. Which pieces are more alike than others depends not only upon what properties they share, but upon who makes the comparison, and when. . . . Circumstances alter similarities.\n\nThis can be easily reversed as an argument in favor of the **potential of similarity analyses to highlight the inter-individual differences in sensory mental representations**. For example, should we ask individuals to judge the similarities in shape or color between various objects, the *differences between the similarity structures* of individuals will be precisely the most important phenomenon for us, far less than the constancy between these structures. If we can account for the context dependence, as we will propose here with explicit instructions, clever task design, and hypothesis-neutral analysis, we could overcome the limitations of the inherently subjective nature of similarity judgements.\n\nThis idea of a difference in similarity judgements in aphantasia seems to transpire in the results of @bainbridgeQuantifyingAphantasiaDrawing2021 on their drawing study. They have shown that aphantasics had more schematic representations during recall, accurate in their spatial positioning, but with less sensory details. This difference can be seen from two perspectives: (1) a memory deficit for sensory properties; (2) a different representational structure of the items in their psychological spaces. In the latter case, aphantasics would have greater/faster abstraction of their representation of a perceived scene, reducing the amount of encoded sensory details unconsciously considered to be relevant. Both (1) and (2) can theoretically explain the same behavioural response, i.e. less sensory elements and correct spatial recall accuracy in aphantasic drawings, but **the two have drastically different consequences on how we define, characterize, and judge aphantasia.**\n\nThe dominant hypothesis seems to be that aphantasics simply have an episodic or general memory deficit. Conversely, I hypothesize that aphantasics have different representational structures than phantasics in certain dimensions of their psychological spaces (notably sensory, but potentially abstract too). More generally, I hypothesize that the concept of visual imagery evaluates in reality the continuous spectrum of representational structures in *sensory* dimensions of psychological spaces. Mirroring visual imagery, spatial imagery could also be a rough psychometric evaluation of the continuous spectrum of structural differences in *conceptual/abstract* dimensions of psychological spaces. In this view, the psychological space of aphantasics would constrain internal representations to particularly abstract forms from a very early stage, thus selectively limiting the item properties thereafter encoded in long-term memory. In other terms, **I hypothesize that aphantasia would not be characterized by an episodic memory deficit, but by an episodic memory *selectivity* caused by the specific characteristics of their representational structures and psychological spaces.** This selectivity would have, as we already hypothesized several times, benefits and drawbacks.\n\n@gardenforsConceptualSpacesFramework2004 proposed that differences in psychological (in his terms, conceptual) spaces could arise from various sources, whether innate, due to learning, or broader cultural or social differences. All these hypotheses could be coherent to explain the sources of aphantasia. Nevertheless, the study of these sources should be the subject of very large-scale or longitudinal studies, which are out of the scope of this project.\n\nHere, we shall rather attempt to **develop a method to characterize the differences in aphantasics' representational structures and psychological spaces.**\n\n# Methodology\n\n@roads2024, in a recent review on the state and perspectives of similarity research, highlighted two challenges that studies in this field had to face: (1) The high cost of collecting behavioral data on a large number of stimuli; (2) The lack of software packages being a high barrier to entry, making the task of coding models difficult for the uninitiated.\n\nTo solve these problems, we present here two solutions, respectively for (1) experimental design and (2) data analysis:\n\n1.  A recent method to efficiently acquire similarity judgements, the \"multiple arrangement of items\" and \"inverse multidimensional scaling\" developed by @kriegeskorteInverseMDSInferring2012.\n2.  An accessible and robust Python toolbox provided by @sasakiToolboxGromovWassersteinOptimal2023 to conduct unsupervised alignment analysis using Gromov-Wasserstein optimal transport.\n\n## Experimental design\n\n### Multi-arrangement and inverse multidimensional scaling\n\nAssuming a geometric model of representational similarities, @kriegeskorteInverseMDSInferring2012 developed a multi-arrangement (MA) method to efficiently acquire (dis)similarity judgments for large sets of objects. The subject has to perform multiple arrangements of item subsets adaptively designed for optimal measurement efficiency and for estimating the representational dissimilarity matrix (RDM) by combining the evidence from the subset arrangements.\n\nThe procedure is illustrated in @fig-multi-arrangement.\n\n![**Acquiring similarity judgements with the multi-arrangement method. (A)** Subjects are asked to arrange items according to their similarity, using mouse drag-and-drop on a computer. The similarity measure is taken as the distances between the items: similar items are closer, while dissimilar items are further apart. The upper part of the figure shows screenshots at different moments of the acquisition for one subject. Columns are trials and rows show the object arrangements over time, running from the start (top row) to the end (last row). The first trial contains all items; subsequent trials contain subsets of items that are adaptively selected to optimally estimate judged similarity for each subject. **(B)** Once acquisition of the final judgements is completed, inter-item distances in the final trial arrangements are combined over trials by rescaling and averaging to yield a single dissimilarity estimate for each object pair. The process is illustrated in this figure for two example item pairs: a boy's face and a hand (red), and carrots and a stop sign (blue). Their single-trial dissimilarity estimates (arrows) are combined into a single dissimilarity estimate, which is placed at the corresponding entry of the RDM (lower panel). Mirror-symmetric entries are indicated by lighter colors. Figure taken from @murHumanObjectSimilarityJudgments2013.](images/multi-arrangement-method-mur-2013.png){#fig-multi-arrangement .column-body}\n\nA key strength of this method that sets it as particularly effective is the \"adaptive\" part. The goal of the process is to acquire similarity judgements as precisely as possible while minimizing the total amount of trials. To do so, starting from the second trial, selected subsets of the items to be compared are presented to the subject: these items are the ones that were very close on-screen in previous trials and thus had their distance evaluated with lower accuracy by the subject. As the subject has to fill the entire \"arena\" with the items, these subsequent trials will necessarily increase the level of precision in the similarity judgement between pairs of items. The second key benefit of this method is the time and effort gain compared to others. For example, to compare every pair of items among 64 different items would require $\\frac{64 \\times (64-1)}{2} = 2016$ comparisons (i.e. trials). This would be extremely time-consuming, while also losing the *context-independence* afforded by the MA method due to the presence of other items around every time the subject mentally performs a pairwise comparison.\n\nHistorically, when referring to the projection of the representations of stimuli (e.g., coordinates in geometric space) from a high-dimensional space into a lower-dimensional space, inference algorithms were commonly called multidimensional scaling [@roads2024]. By analogy, the process of combining several lower-dimensional (2D) similarity judgements on-screen to form one higher dimensional similarity representation (in the RDM) can be conceptually seen as \"inverse\" multidimensional scaling, hence the name given to the method by @kriegeskorteInverseMDSInferring2012.\n\n### Principle\n\nThe idea is simple: for a given set of items that have distinct and very pictorial visual properties, we would ask a wide range of aphantasics, phantasics or hyperphantasics to imagine, mentally compare and make similarity judgements between the items. To compare these representations with actual perceptual representations, the subjects would also perform the same task afterwards, this time with actual pictures to compare. Subjects would also fill our usual psychometric imagery questionnaires.\n\nTo \"compare imagined items\", we could use a \"word\" version of the MA paradigm. An example from @majewskaSpatialMultiarrangementClustering2020 - *who used the method to build large-scale semantic similarity resources for Natural Language Processing systems* - is represented in @fig-majewska.\n\n![Arena layout of the MA protocol used by @majewskaSpatialMultiarrangementClustering2020 to acquire similarity judgements on word pairs.](images/majewska-spam.png){#fig-majewska width=\"50%\"}\n\n## Hypotheses\n\n### Aphantasic and phantasic psychological spaces\n\nThe most representative members of a category are called prototypical members.\n\nPrototype theory builds on the observation that among the instances of a property, some are more representative than others. The most representative one is the prototype of the property.\n\nThus, following the concepts illustrated by Gardenfors, we would expect that aphantasics, when doing shape similarity judgements, would be more inclined to group items close to the prototypical items due to a lower definition of the mental image. In comparison, phantasics would have a much more distributed conceptual space of item shapes due to their higher-resolution mental images of said items.\n\n### Subjective imagery and psychological spaces\n\nIn the proposed view of visual imagery as the subjective expression of a given type of psychological space, we mentioned earlier that *spatial* imagery could also constitute a subjective expression of other dimensions of psychological spaces. Hence, the *verbal* dimension of the simplified model of imagery we outlined in my thesis project could also represent different dimensions.\n\nThis conception leads to the following theoretical hypothesis: provided that our visual-spatial-verbal model correctly fits subjective imagery, the imagery profile of individuals should map on their psychological spaces.\n\nOperationally, this would be evaluated by the fact that **individuals with similar imagery profiles** (visual, spatial, verbal, or any combination of the three) **should have similar representations** in their given psychological space, **quantifiable by the degree of alignment between their similarity structures.**\n\n![The two conditions for one subject.](images/my-protocol-1.png)\n\n![The comparison between the representational structure of aphantasics and phantasics. This figure illustrates the principle, but in reality all pairs of subjects will be compared to assess their representational structure alignment. This is computationnally heavy, but analytically very powerful.](images/my-protocol-2.png)\n\n<!-- Summary schematics of the proposed experimental protocol and data analysis plan. -->\n\n# Study data simulation and analysis\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ═══ Packages ═════════════════════════════════════════════════════════════════\n\nif (!require(librarian)) install.packages(librarian) \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLe chargement a nécessité le package : librarian\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(librarian)                                     \n\n# now putting packages on our library's shelves:\nshelf(\n  # ─── data management ─────────────────\n  holodeck,       # simulating multivariate data\n  cluster,        # dissimilarity matrices\n  \n  # ─── modelling ───────────────────────\n  mclust,         # mixture clustering\n  \n  #  data visualization ──────────────\n  # palettes\n  viridis,        # colour-blind friendly palettes\n  # interactive\n  plotly,         # interactive plots\n  ggdendro,       # dendrograms\n  seriation,      # dissimilarity plots\n  webshot2,       # HTML screenshots for Word render\n  webshot,\n  \n  # ─── essential package collections ───\n  doParallel,     # parallel execution\n  easystats,      # data analysis ecosystem\n  reticulate,     # R to Python                    \n  tidyverse,      # modern R ecosystem\n)\n\n# ─── Global cosmetic theme ───\ntheme_set(theme_modern(base_size = 14))\n\npal_okabe_ito <- c(     # <3>                                                \n  \"#E69F00\", \"#56B4E9\", \"#009E73\",                            \n  \"#F5C710\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")      \n\n# We'll need 9 colors at some point\npal_okabe_ito_extended <- c(                                 \n  \"#E69F00\", \"#56B4E9\", \"#009E73\",                           \n  \"#F5C710\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\", \"#414487FF\")\n\n# We'll need 30 colors at another moment\ncool_30_colors <- c(                                                   \n  \"#3d51b4\", \"#414487FF\", \"#003d73\", \"#440154FF\", \"#6c0009\", \"#b64e4e\",\n  \"#D55E00\", \"#E69F00\", \"#F5C710\", \"#FDE725FF\", \"#f2bb7b\", \"#f1afad\", \"#CC79A7\", \n  \"#e57774\", \"#7AD151FF\", \"#57b571\", \"#318a4a\", \"#009E73\", \"#22A884FF\", \n  \"#2A788EFF\", \"#0072B2\", \"#2da6b5\", \"#56B4E9\", \"#889be0\", \"#6677e0\",   \n  \"#3d51b4\", \"#414487FF\", \"#003d73\", \"#440154FF\", \"#6c0009\", \"#b64e4e\"  \n  ) # <3>                                                                      \n\npath = \"notebooks/data/\"\n\ndf <- read_rds(paste0(path, \"df.RDS\"))\ndf_embeds <- read_rds(paste0(path, \"df_embeds.RDS\"))\n# Categorical and visual embeddings\ndf_embeds_categ  <- read_rds(paste0(path, \"df_embeds_categ.RDS\"))\ndf_embeds_visual <- read_rds(paste0(path, \"df_embeds_visual.RDS\"))\n# Subject embeddings per sub-group\ndf_embed_c_sub  <- read_rds(paste0(path, \"df_embed_c_sub.RDS\"))\ndf_embed_cs_sub <- read_rds(paste0(path, \"df_embed_cs_sub.RDS\"))\ndf_embed_v_sub  <- read_rds(paste0(path, \"df_embed_v_sub.RDS\"))\ndf_embed_vs_sub <- read_rds(paste0(path, \"df_embed_vs_sub.RDS\"))\n# Accuracy of the unsupervised alignment (bad = not tidy data)\ndf_accuracy_all_bad <- read_rds(paste0(path, \"df_accuracy_all_bad.RDS\"))\ndf_accuracy_cat_bad <- read_rds(paste0(path, \"df_accuracy_cat_bad.RDS\"))\n# Coordinates of the aligned embeddings from the Python output\ncoordinates_aligned_embeddings <- read_rds(paste0(path, \"coordinates_aligned_embeddings.RDS\"))\n```\n:::\n\n\n## Visual-spatial-verbal model of cognitive profiles\n\nOne of the objectives of the study would be to link the subjective cogntive profiles of individuals with their representational structures. To evaluate these profiles, we are going to use psychometric questionnaires evaluating the visual-object, spatial, and verbal dimensions of imagery which will yield three scores, one for each dimension.\n\nWe are going to simulate 30 participants presenting four different cognitive profiles, that I defined as, respectively, *verbal* aphantasics, *spatial* aphantasics, *spatial* phantasics, and *visual* phantasics. Their imagery abilities are summarised in @tbl-imageries.\n\nTo simulate these four sub-groups, we use the `holodeck` R package to generate multivariate normal distributions of scores on these three dimensions for each sub-group. For instance, verbal aphantasics have normally distributed visual imagery scores centered around a mean of 0 (normalized, so negative scores are possible), 0.4 for spatial imagery, and 0.7 for verbal style; Spatial aphantasics have means of 0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are arbitrary, but have been chosen by trial-and-error to obtain a model that is both well-defined and not exagerrated. The 30 subjects' imagery profiles are represented in the three dimensional space of the visual-spatial-verbal dimensions in @fig-plot_osv_model.\n\n| Cognitive profile  | Visual imagery | Spatial imagery | Verbal style |\n|--------------------|:--------------:|:---------------:|:------------:|\n| Verbal aphantasic  |       --       |       \\-        |      ++      |\n| Spatial aphantasic |       --       |       ++        |      \\-      |\n| Spatial phantasic  |       \\+       |       ++        |      \\-      |\n| Visual phantasic   |       ++       |       \\-        |      \\+      |\n\n: Imagery abilities of the four hypothesized cognitive profiles. {#tbl-imageries}\n\nDown below is the code to generate these scores.\n\n\n{{< embed notebooks/simulation-code.qmd#osv_model >}}\n\n\n::: {#cell-fig-plot_osv_model .cell}\n\n```{.r .cell-code .hidden}\nplotting_osv_model <- function(df, grouping_variable, size){\n  df |> \n    plot_ly(\n      x = ~visual_imagery,\n      y = ~spatial_imagery,\n      z = ~verbal_profile,\n      color = ~df[[grouping_variable]],\n      text  = ~df[[grouping_variable]],\n      colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\"),\n      type = \"scatter3d\",\n      mode = \"markers+text\",\n      marker = list(size = size),\n      textfont = list(size = size + 4)\n    ) |> \n    layout(\n      scene = list(\n        xaxis = list(\n          title = list(text = \"Visual imagery\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          ),\n        yaxis = list(\n          title = list(text = \"Spatial imagery\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          ),\n        zaxis = list(\n          title = list(text = \"Verbal profile\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          )\n      ),\n      legend = list(title = list(text = \"Group\")),\n      paper_bgcolor = \"transparent\"\n      )\n}\n\ndf |> \n  mutate(vis_spa_group = case_when(\n    vis_spa_group == \"aph_spa_high\" ~ \"Aph. spatial\",\n    vis_spa_group == \"aph_spa_low\"  ~ \"Aph. verbal\",\n    vis_spa_group == \"phant_spa_high\" ~ \"Phant. spatial\",\n    vis_spa_group == \"phant_spa_low\"  ~ \"Phant. visual\"\n  )) |> \n  plotting_osv_model(grouping_variable = \"vis_spa_group\", size = 4)\n```\n\n::: {#fig-plot_osv_model .cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-6f73220719663c7b9b63\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-6f73220719663c7b9b63\">{\"x\":{\"visdat\":{\"692411f854a9\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"692411f854a9\",\"attrs\":{\"692411f854a9\":{\"x\":{},\"y\":{},\"z\":{},\"text\":{},\"mode\":\"markers+text\",\"marker\":{\"size\":4},\"textfont\":{\"size\":8},\"color\":{},\"colors\":[\"#E69F00\",\"#56B4E9\",\"#009E73\",\"#F5C710\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":{\"text\":\"Visual imagery\",\"font\":{\"color\":\"grey\"}},\"tickfont\":{\"color\":\"grey\"}},\"yaxis\":{\"title\":{\"text\":\"Spatial imagery\",\"font\":{\"color\":\"grey\"}},\"tickfont\":{\"color\":\"grey\"}},\"zaxis\":{\"title\":{\"text\":\"Verbal profile\",\"font\":{\"color\":\"grey\"}},\"tickfont\":{\"color\":\"grey\"}}},\"legend\":{\"title\":{\"text\":\"Group\"}},\"paper_bgcolor\":\"transparent\",\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[-0.036640408157145976,0.032135165776453471,-0.23114036852084763,-0.03286385746881252,0.46365788701643423,0.046246978019755067,-0.15990438091959489],\"y\":[0.8092596649106395,0.9181050980868094,0.97440441509825804,1.0024534880692733,0.69229079295427742,0.89782001779250031,0.70406385738737387],\"z\":[0.23767465902410329,0.48935370636711645,0.16138488517155131,0.26781100284524345,0.27163625306646155,0.099616322742655999,0.1436690095419996],\"text\":[\"Aph. spatial\",\"Aph. spatial\",\"Aph. spatial\",\"Aph. spatial\",\"Aph. spatial\",\"Aph. spatial\",\"Aph. spatial\"],\"mode\":\"markers+text\",\"marker\":{\"color\":\"rgba(230,159,0,1)\",\"size\":4,\"line\":{\"color\":\"rgba(230,159,0,1)\"}},\"textfont\":{\"color\":\"rgba(230,159,0,1)\",\"size\":8},\"type\":\"scatter3d\",\"name\":\"Aph. spatial\",\"error_y\":{\"color\":\"rgba(230,159,0,1)\"},\"error_x\":{\"color\":\"rgba(230,159,0,1)\"},\"line\":{\"color\":\"rgba(230,159,0,1)\"},\"frame\":null},{\"x\":[-0.025429768636669441,0.20540568530003891,0.39728317988015294,0.14173565715250605,0.046671484230633879,-0.091550981754586078,0.24115219376733835,-0.0031252223462148957],\"y\":[0.28768025699895849,0.66052940881134692,0.51856860955690887,0.38961701935818965,0.1405293057853374,0.37274729653807609,0.25503830665271376,0.23688592302030451],\"z\":[0.64016397019294979,0.95193303931403672,0.80447184300766228,0.63985204409744911,0.8823914052752575,0.65998947099625216,0.81018776299107276,0.97201145308433856],\"text\":[\"Aph. verbal\",\"Aph. verbal\",\"Aph. verbal\",\"Aph. verbal\",\"Aph. verbal\",\"Aph. verbal\",\"Aph. verbal\",\"Aph. verbal\"],\"mode\":\"markers+text\",\"marker\":{\"color\":\"rgba(86,180,233,1)\",\"size\":4,\"line\":{\"color\":\"rgba(86,180,233,1)\"}},\"textfont\":{\"color\":\"rgba(86,180,233,1)\",\"size\":8},\"type\":\"scatter3d\",\"name\":\"Aph. verbal\",\"error_y\":{\"color\":\"rgba(86,180,233,1)\"},\"error_x\":{\"color\":\"rgba(86,180,233,1)\"},\"line\":{\"color\":\"rgba(86,180,233,1)\"},\"frame\":null},{\"x\":[0.74481985224987479,0.63269072880560084,0.69925449101644432,0.65422881534251864,0.81601255966431641,0.81229196076556276,0.50160614878139609],\"y\":[0.67867666218266998,0.27831000980543458,0.52182701228762196,0.50943441424031544,0.61438500079481451,0.91029741083467663,0.76711977618888827],\"z\":[0.45913811760608259,0.3244745718620089,0.24346070415584314,0.01744647135913846,0.10174591033524269,0.25380696822500592,0.17896423253579011],\"text\":[\"Phant. spatial\",\"Phant. spatial\",\"Phant. spatial\",\"Phant. spatial\",\"Phant. spatial\",\"Phant. spatial\",\"Phant. spatial\"],\"mode\":\"markers+text\",\"marker\":{\"color\":\"rgba(0,158,115,1)\",\"size\":4,\"line\":{\"color\":\"rgba(0,158,115,1)\"}},\"textfont\":{\"color\":\"rgba(0,158,115,1)\",\"size\":8},\"type\":\"scatter3d\",\"name\":\"Phant. spatial\",\"error_y\":{\"color\":\"rgba(0,158,115,1)\"},\"error_x\":{\"color\":\"rgba(0,158,115,1)\"},\"line\":{\"color\":\"rgba(0,158,115,1)\"},\"frame\":null},{\"x\":[0.67831641540239429,1.1422797876722326,0.81262592711973258,1.060174306829138,0.662557283931551,1.0736825733106234,0.94234040029112065,1.0334903414928389],\"y\":[0.45873604497674658,0.52907881530186329,0.66152156986357102,0.0077177840797507113,0.32899576237076489,0.30893873337987554,0.44331306526420589,0.25071725641675868],\"z\":[0.66867015783560002,0.53043379043594585,0.26623635715828653,0.68633684399556916,0.34336243473597405,0.73504592448457806,0.64907679257819506,0.23109420803880515],\"text\":[\"Phant. visual\",\"Phant. visual\",\"Phant. visual\",\"Phant. visual\",\"Phant. visual\",\"Phant. visual\",\"Phant. visual\",\"Phant. visual\"],\"mode\":\"markers+text\",\"marker\":{\"color\":\"rgba(245,199,16,1)\",\"size\":4,\"line\":{\"color\":\"rgba(245,199,16,1)\"}},\"textfont\":{\"color\":\"rgba(245,199,16,1)\",\"size\":8},\"type\":\"scatter3d\",\"name\":\"Phant. visual\",\"error_y\":{\"color\":\"rgba(245,199,16,1)\"},\"error_x\":{\"color\":\"rgba(245,199,16,1)\"},\"line\":{\"color\":\"rgba(245,199,16,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n\nImagery profiles generated for 30 subjects on the three object, spatial, and verbal dimensions.\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.10.4/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}